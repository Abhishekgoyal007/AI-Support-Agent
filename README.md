# ğŸ›’ TechNest AI Support Agent

An AI-powered customer support chat widget built for Spur's take-home assignment. Features real-time chat with Google Gemini 2.5 Flash, conversation persistence, and a modern dark theme UI.

![TechNest Support](https://img.shields.io/badge/AI-Gemini%202.5-blue) ![React](https://img.shields.io/badge/React-18-61DAFB) ![Node.js](https://img.shields.io/badge/Node.js-Express-339933) ![SQLite](https://img.shields.io/badge/Database-SQLite-003B57)

## âœ¨ Features

- ğŸ¤– **AI-Powered Chat** - Powered by Google Gemini 2.5 Flash
- ğŸ’¬ **Real-time Responses** - Instant AI replies with typing indicators
- ğŸ“ **Conversation Persistence** - Chat history saved across sessions
- ğŸ¨ **Modern Dark Theme** - Beautiful, responsive UI
- ğŸ“± **Mobile Friendly** - Works on all screen sizes
- ğŸ›¡ï¸ **Rate Limiting** - Prevents API abuse
- âœ… **Input Validation** - Zod-based validation

## ğŸª About TechNest (Fictional Store)

TechNest is a premium electronics & gadgets e-commerce store. The AI agent can answer questions about:

| Topic | Information |
|-------|-------------|
| ğŸ“¦ Shipping | Free over $50, Standard (5-7 days), Express (2-3 days) |
| ğŸ”„ Returns | 30-day hassle-free return policy |
| ğŸ’³ Payments | Visa, Mastercard, PayPal, Apple Pay, Klarna |
| ğŸ›¡ï¸ Warranty | 1-year manufacturer warranty |
| â° Support | Mon-Fri 9AM-6PM, Sat 10AM-4PM EST |

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| Frontend | React 18 + Vite + TypeScript |
| Styling | Vanilla CSS (Dark theme) |
| Backend | Node.js + Express + TypeScript |
| Database | SQLite + Prisma ORM |
| LLM | Google Gemini 2.5 Flash |
| Validation | Zod |

## ğŸ“ Project Structure

```
spur-ai-support-agent/
â”œâ”€â”€ client/                 # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ hooks/          # useChat hook
â”‚   â”‚   â”œâ”€â”€ services/       # API client
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript types
â”‚   â”‚   â”œâ”€â”€ App.tsx         # Main component
â”‚   â”‚   â””â”€â”€ App.css         # Styles
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ server/                 # Node.js Backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ routes/         # API routes
â”‚   â”‚   â”œâ”€â”€ services/       # LLM, Conversation services
â”‚   â”‚   â”œâ”€â”€ middleware/     # Validation, Error handling
â”‚   â”‚   â””â”€â”€ index.ts        # Entry point
â”‚   â”œâ”€â”€ prisma/
â”‚   â”‚   â”œâ”€â”€ schema.prisma   # Database schema
â”‚   â”‚   â””â”€â”€ seed.ts         # FAQ seed data
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ package.json            # Monorepo root
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- **Node.js 18+** 
- **npm**
- **Google Gemini API Key** - Get one at [Google AI Studio](https://aistudio.google.com/app/apikey)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/Abhishekgoyal007/AI-Support-Agent.git
cd AI-Support-Agent

# 2. Install all dependencies
npm install

# 3. Set up environment variables
cp server/.env.example server/.env
```

### Configure API Key

Edit `server/.env` and add your Gemini API key:

```env
GEMINI_API_KEY=your_gemini_api_key_here
PORT=3001
NODE_ENV=development
DATABASE_URL="file:./dev.db"
CLIENT_URL=http://localhost:5173
```

### Initialize Database

```bash
# Run migrations
npm run db:migrate

# Seed FAQ data
npm run db:seed
```

### Start Development

```bash
# Start both frontend and backend
npm run dev

# Or start separately:
npm run dev:server  # Backend on http://localhost:3001
npm run dev:client  # Frontend on http://localhost:5173
```

Open [http://localhost:5173](http://localhost:5173) in your browser.

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/chat/message` | Send a message and get AI response |
| GET | `/api/chat/history/:sessionId` | Get conversation history |
| POST | `/api/chat/session` | Create new chat session |
| GET | `/health` | Health check |

### Example Request

```bash
curl -X POST http://localhost:3001/api/chat/message \
  -H "Content-Type: application/json" \
  -d '{"message": "What are your shipping options?"}'
```

### Example Response

```json
{
  "reply": "We offer FREE shipping on orders over $50. Standard shipping takes 5-7 business days...",
  "sessionId": "uuid-here"
}
```

## ğŸ§ª Testing

### Manual Testing Checklist

- [ ] Send a message and receive AI response
- [ ] Check conversation history persists after refresh
- [ ] Click suggestion buttons for quick questions
- [ ] Test "New Chat" button clears conversation
- [ ] Verify error toast appears on network failure
- [ ] Test on mobile/tablet screen sizes

### Test Scenarios

1. **Shipping Questions**: "What are your shipping options?"
2. **Returns**: "What is your return policy?"
3. **Payments**: "What payment methods do you accept?"
4. **Support Hours**: "What are your support hours?"
5. **Promotions**: "Do you have any discounts?"

## ğŸ—ï¸ Architecture Overview

### Backend Structure

```
server/src/
â”œâ”€â”€ index.ts              # Express app entry point
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ prisma.ts         # Prisma client singleton
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ chat.ts           # Chat API endpoints
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ conversationService.ts  # CRUD for conversations/messages
â”‚   â”œâ”€â”€ knowledgeService.ts     # FAQ retrieval for prompts
â”‚   â””â”€â”€ llmService.ts           # Gemini API integration
â””â”€â”€ middleware/
    â”œâ”€â”€ errorHandler.ts   # Global error handling
    â”œâ”€â”€ validation.ts     # Zod input validation
    â””â”€â”€ requestLogger.ts  # Request logging
```

### Design Decisions

**1. Service Layer Pattern**
- Business logic is encapsulated in services (`llmService`, `conversationService`)
- Routes are thin controllers that delegate to services
- Makes it easy to add new channels (WhatsApp, Instagram) by reusing services

**2. LLM Integration**
- `LLMService` class encapsulates all Gemini API interactions
- System prompt includes dynamically fetched FAQ knowledge from database
- Conversation history (last 10 messages) passed for context
- Graceful fallback to mock responses when API key is missing

**3. Database Schema**
```
Conversation (1) â”€â”€â†’ (N) Message
                          â”œâ”€â”€ sender: "user" | "ai"
                          â”œâ”€â”€ text: string
                          â””â”€â”€ tokenCount: int (optional)

KnowledgeBase
â”œâ”€â”€ category: "shipping" | "returns" | etc.
â”œâ”€â”€ question: string
â”œâ”€â”€ answer: string
â””â”€â”€ priority: int (for ordering)
```

**4. Session Management**
- Session ID stored in localStorage on frontend
- New session created automatically on first message
- History loaded on page refresh via GET `/api/chat/history/:sessionId`

### LLM Prompting Strategy

```
System Prompt Structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Persona: Friendly support agent     â”‚
â”‚ Guidelines: Concise, helpful, etc.  â”‚
â”‚ FAQ Knowledge: Dynamically loaded   â”‚
â”‚ Constraints: No markdown, plain txt â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conversation History (last 10 msgs) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Current User Message                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why These Tech Choices?

| Choice | Reasoning |
|--------|-----------|
| **Gemini 2.5 Flash** | Fast (1-2s responses), cost-effective, generous free tier |
| **SQLite + Prisma** | Zero-config, portable, type-safe ORM, easy PostgreSQL migration |
| **React + Vite** | Fast dev experience, familiar ecosystem, TypeScript support |
| **Monorepo** | Single `npm install`, simplified deployment, shared configs |
| **Zod** | Runtime validation with TypeScript inference |

## ğŸ”® Trade-offs & If I Had More Time

### Current Trade-offs

| Decision | Trade-off |
|----------|-----------|
| SQLite over PostgreSQL | Simpler setup, but no concurrent writes for production |
| No WebSocket | Polling-based, but simpler architecture for demo |
| No authentication | Simpler UX, but anyone can access any session with ID |
| Single LLM provider | Locked to Gemini, but cleaner code without abstraction |

### If I Had More Time...

**Features I'd Add:**
- ğŸ” **User Authentication** - OAuth with Google/GitHub
- ğŸ”Œ **WebSocket Support** - Real-time streaming responses
- ğŸ“Š **Admin Dashboard** - View all conversations, analytics
- ğŸŒ **Multi-language Support** - i18n for UI and AI responses
- ğŸ¨ **Theme Customization** - Light mode, custom branding
- ğŸ“ **File Uploads** - Share images/documents with support

**Technical Improvements:**
- âš¡ **Response Streaming** - Stream LLM tokens for faster perceived response
- ğŸ§ª **Unit Tests** - Jest/Vitest for services and API endpoints
- ğŸ“ˆ **Observability** - OpenTelemetry tracing, structured logging
- ğŸ”„ **LLM Fallback** - Automatic failover to OpenAI if Gemini is down
- ğŸ—ƒï¸ **PostgreSQL** - For production scalability
- ğŸ³ **Docker** - Containerized deployment

**UX Improvements:**
- âœï¸ **Message Editing** - Edit sent messages
- ğŸ‘ **Feedback Buttons** - Thumbs up/down on AI responses
- ğŸ“‹ **Copy Message** - One-click copy for AI responses
- ğŸ”Š **Sound Notifications** - Audio feedback for new messages

## ğŸ“ Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `GEMINI_API_KEY` | Google Gemini API key | Required |
| `PORT` | Backend server port | 3001 |
| `NODE_ENV` | Environment | development |
| `DATABASE_URL` | SQLite database path | file:./dev.db |
| `CLIENT_URL` | Frontend URL for CORS | http://localhost:5173 |

## ğŸš¢ Deployment

### Frontend (Vercel)

1. Connect GitHub repo to Vercel
2. Set root directory to `client`
3. Build command: `npm run build`
4. Output directory: `dist`

### Backend (Render)

1. Create new Web Service on Render
2. Connect GitHub repo
3. Set root directory to `server`
4. Build command: `npm install && npx prisma generate && npx prisma migrate deploy`
5. Start command: `npm start`
6. Add environment variables

## ğŸ‘¤ Author

**Abhishek Ashok Goyal**

- GitHub: [@Abhishekgoyal007](https://github.com/Abhishekgoyal007)

## ğŸ“„ License

MIT License - feel free to use this project as a reference!

---

Built with â¤ï¸ for Spur
